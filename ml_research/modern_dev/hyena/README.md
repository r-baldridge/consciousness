# Hyena - Long Convolutions for Sequence Modeling

> A sub-quadratic replacement for attention using implicit long convolutions, achieving up to 100x speedup over Transformers at sequence length 64K while maintaining competitive quality.

**Status:** Scaffolding Complete - Implementation Pending
**Paper:** [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
**Year:** 2023
**Organization:** Hazy Research / Stanford University

## Overview

Hyena introduces a fundamentally different approach to sequence modeling by replacing attention with implicit long convolutions and data-controlled gating. Rather than computing pairwise interactions between all tokens (O(n^2)), Hyena uses FFT-based convolutions (O(n log n)) with learned filters that can span the entire sequence length without materializing the full attention matrix.

The key insight is that convolution filters can be parameterized implicitly through small neural networks that generate filter values from positional encodings. This allows constant parameter count regardless of sequence length, smooth learnable filter shapes, and the ability to extrapolate to longer sequences than seen during training. Combined with data-dependent gating mechanisms, Hyena achieves expressivity comparable to attention.

The Hyena Hierarchy stacks multiple Hyena operators recursively, where each level adds another order of data control. An order-N Hyena has N multiplicative interactions, similar in expressivity to N-head attention but with sub-quadratic complexity. This makes Hyena particularly well-suited for extremely long sequences, with demonstrated 100x speedup at 64K tokens where standard attention runs out of memory.

## Key Innovations

- **Implicit Long Convolutions**: Convolution filters generated by small MLPs from positional encodings, enabling arbitrary-length filters with constant parameter count.

- **Data-Controlled Gating**: Multiplicative interactions between branches combine short convolution and element-wise gating for context-dependent modulation.

- **Hyena Hierarchy**: Recursive stacking of operators provides N-th order interactions, matching attention expressivity with O(n log n) complexity.

## Architecture Diagram

```
Standard Attention:
    Q, K, V = Linear(x), Linear(x), Linear(x)
    Attn(Q, K, V) = softmax(Q @ K^T / sqrt(d)) @ V
    Complexity: O(L^2 * d)

Hyena Operator (Order N):
    v = Linear(x)                           # Value projection
    x_1, x_2, ..., x_N = Linear(x)          # Control signals

    For i = 1 to N:
        h_i = ImplicitConv(positional_encoding)  # Long convolution
        v = x_i * (h_i * v)                      # Gated convolution

    y = Linear(v)                           # Output projection
    Complexity: O(L log L) per operator

                        Input x
                           |
              +------------+------------+
              |            |            |
              v            v            v
           Linear       Linear       Linear
              |            |            |
              v            v            v
           x_1, x_2      ...          v (value)
              |                        |
              +----> Gate <----+       |
                       |               |
                       v               v
                  Long Conv  <---------+
                  (FFT-based)
                       |
                       v
                   Output y
```

## Current Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| ImplicitFilter | Stub | MLP-based filter generation |
| FFTConv | Stub | FFT-based long convolution |
| HyenaOperator | Stub | Single layer (order 2) |
| HyenaBlock | Stub | With normalization |
| PositionalEncoding | Stub | Sinusoidal embeddings |
| Training Loop | Stub | CLI ready |
| Tests | Ready | Import tests |
| Configs | Ready | Standard and long-context |

## Hyena Variants

| Variant | Description | Paper |
|---------|-------------|-------|
| Hyena | Original long convolution model | [arXiv:2302.10866](https://arxiv.org/abs/2302.10866) |
| HyenaDNA | Genomics-focused for DNA sequences | [arXiv:2306.15794](https://arxiv.org/abs/2306.15794) |
| SE(3)-Hyena | Equivariant variant for 3D point clouds | Research in progress |

## Requirements for Full Implementation

### Dependencies
- PyTorch >= 2.0
- einops
- opt-einsum
- scipy (for FFT)
- cauchy_mult (custom CUDA kernel, optional)

### Hardware
- GPU recommended for training
- Memory efficient: O(n log n) vs O(n^2)
- Can process 64K+ sequences on single GPU

### External Resources
- [ ] Official implementation: [HazyResearch/safari](https://github.com/HazyResearch/safari)
- [ ] HyenaDNA: [HazyResearch/hyena-dna](https://github.com/HazyResearch/hyena-dna)
- [ ] Training data: The Pile for language modeling

## Quick Start

```python
from consciousness.ml_research.modern_dev.hyena import DEFAULT_CONFIG, LONG_CONTEXT_CONFIG

# Default Configuration
config = {
    "d_model": 512,
    "n_layer": 12,
    "order": 2,
    "filter_order": 64,
    "emb_dim": 3,
    "short_filter_order": 3,
    "dropout": 0.0,
    "activation": "gelu",
    "bidirectional": False,
}

# Long Context Configuration (65K tokens)
long_config = {
    "d_model": 512,
    "n_layer": 8,
    "order": 2,
    "filter_order": 128,
    "emb_dim": 5,
    "max_seq_len": 65536,
    "use_flash_fft": True,
}

# When implemented:
# from consciousness.ml_research.modern_dev.hyena.src.operator import HyenaOperator
# layer = HyenaOperator(**config)
```

## File Structure

```
hyena/
├── __init__.py       # Module documentation and variant metadata
├── README.md         # This file
├── src/
│   ├── filter.py     # ImplicitFilter and ExponentialWindow
│   ├── fft_conv.py   # FFTConv and FlashFFTConv
│   ├── operator.py   # HyenaOperator and HyenaBlock
│   └── model.py      # HyenaLM and HyenaEncoder
├── configs/
│   ├── hyena.yaml        # Standard configuration
│   ├── hyena_long.yaml   # Long context (64K+)
│   └── se3_hyena.yaml    # SE(3) equivariant
├── cli/
│   ├── train.py      # Training script (hyena-train)
│   ├── bench.py      # Speed/memory benchmarks
│   └── visualize.py  # Filter visualization
├── models/           # Pretrained checkpoints
├── docs/             # Additional documentation
└── tests/
    └── test_model.py # Unit tests
```

## Mathematical Formulation

**Hyena Hierarchy Formula:**
```
Order 1:
    H^(1)(v, x_1) = (h * v) . x_1

Order N (recursive):
    H^(N)(v, x_1, ..., x_N) = H^(1)(H^(N-1)(v, x_1, ..., x_{N-1}), x_N)

Explicit Order 2:
    H^(2)(v, x_1, x_2) = ((h_2 * ((h_1 * v) . x_1)) . x_2)

Where:
    . = element-wise multiplication
    * = long convolution (via FFT)
    h_i = ImplicitFilter_i(positional_encoding)
```

**Implicit Filter:**
```
h(t) = MLP(PE(t)) * Window(t)
PE(t) = [sin(w_1 * t), cos(w_1 * t), ..., sin(w_k * t), cos(w_k * t), t/L]
Window(t) = exp(-alpha * t)  # Exponential decay for stability
```

**FFT Convolution:**
```
h * v = IFFT(FFT(h) . FFT(v))
Complexity: O(L log L)
```

**Computational Complexity:**
```
Per Hyena operator: O(L log L)
Total for depth D: O(D * L log L)
vs Attention: O(L^2)
```

## SE(3)-Hyena for Point Clouds

Extension for 3D point cloud and molecular data with SE(3) equivariance:

```
Input: Point cloud P = {(x_i, f_i)} where x_i in R^3

Spherical Harmonics Encoding:
    Y_l^m(r_ij) = spherical_harmonic(l, m, r_ij / ||r_ij||)
    where r_ij = x_j - x_i

Radial Encoding:
    R(r) = MLP(GaussianBasis(||r_ij||))

SE(3)-Equivariant Convolution:
    f_out^l = Sum_{l1, l2} CG(l1, l2, l) * (h_{l1}(r) tensor f_in^{l2})
    where CG = Clebsch-Gordan coefficients
```

## Benchmarks

**Language Modeling (The Pile):**
- Hyena-355M: Competitive with GPT-style transformers at 2048 context
- Training speedup: 2x at 2K, 10x at 8K

**Long Context:**
- PathX (16K tokens): 94.5% accuracy
- Path256 (65K tokens): First sub-quadratic model to solve
- Attention models: Fail due to memory/compute constraints

**Speed Comparison at 64K Sequence:**

| Method | Relative Speed |
|--------|----------------|
| Hyena | 1x (baseline) |
| FlashAttention | 100x slower |
| Standard Attention | OOM |

## Comparison with Other Architectures

|                    | Hyena | Mamba | Transformer | Linear Attn |
|--------------------|-------|-------|-------------|-------------|
| Complexity | O(L log L) | O(L) | O(L^2) | O(L) |
| Parallel Training | Yes | Limited | Yes | Yes |
| Long Context | Excellent | Excellent | Poor | Good |
| Expressivity | High | High | Highest | Medium |
| Memory (64K) | Low | Lowest | OOM | Low |

## References

- Poli, M., et al. "Hyena Hierarchy: Towards Larger Convolutional Language Models" (2023). arXiv:2302.10866
- Nguyen, E., et al. "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution" (2023). arXiv:2306.15794
- [Safari Repository](https://github.com/HazyResearch/safari)
- Related: S4, LSSL, CKConv

## Contributing

To complete this implementation:

1. **Phase 1: Core Implementation**
   - Implement `ImplicitFilter` with MLP-based generation
   - Create FFT-based long convolution module
   - Build basic HyenaOperator (order 2)
   - Add window functions for filter stability

2. **Phase 2: Full Architecture**
   - Implement HyenaBlock with residual and normalization
   - Add multi-order Hyena hierarchy support
   - Create position-aware gating mechanisms
   - Build language model wrapper

3. **Phase 3: SE(3)-Hyena (Optional)**
   - Implement spherical harmonics encoding
   - Add radial basis functions
   - Create equivariant convolution operations
   - Build point cloud data pipeline

4. **Phase 4: Optimization**
   - Integrate custom CUDA kernels for implicit filters
   - Add FlashFFTConv for maximum speed
   - Enable mixed precision training
