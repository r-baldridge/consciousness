# RWKV Default Configuration
# Reference: "RWKV: Reinventing RNNs for the Transformer Era"

# Model Architecture
model:
  vocab_size: 50277             # Vocabulary size
  hidden_dim: 768               # Model hidden dimension
  num_layers: 12                # Number of RWKV blocks
  head_size: 64                 # Size of each attention head
  context_length: 4096          # Maximum context length
  version: 4                    # RWKV version (4, 5, 6, or 7)

# RWKV-6 Features (data-dependent decay)
rwkv6:
  use_data_dependent_decay: false  # Enable RWKV-6 style decay
  decay_lora_dim: 64               # LoRA dimension for decay projection

# Layer Configuration
layers:
  time_mix_extra_dim: 0         # Extra dimension for time mixing
  channel_mix_extra_dim: 0      # Extra dimension for channel mixing
  rescale_layer: 0              # Rescale every N layers (0 to disable)

# Normalization
normalization:
  epsilon: 1e-5                 # Epsilon for layer normalization

# Initialization
initialization:
  initializer_range: 0.02       # Standard deviation for weight init
  tie_word_embeddings: true     # Tie input and output embeddings

# Regularization
regularization:
  dropout: 0.0                  # Dropout probability

# Training Configuration
training:
  learning_rate: 6e-4           # Peak learning rate
  min_learning_rate: 6e-5       # Minimum learning rate
  weight_decay: 0.1             # Weight decay coefficient
  beta1: 0.9                    # Adam beta1
  beta2: 0.99                   # Adam beta2 (RWKV often uses 0.99)
  grad_clip: 1.0                # Gradient clipping norm

  # Learning rate schedule
  warmup_steps: 0               # RWKV typically doesn't use warmup
  lr_decay_style: "exponential" # LR decay style

  # Batch size
  batch_size: 32                # Micro batch size
  gradient_accumulation_steps: 4 # Gradient accumulation

  # Sequence length
  max_seq_len: 4096             # Maximum sequence length

  # Training duration
  max_steps: 100000             # Maximum training steps

  # Checkpointing
  save_interval: 1000           # Save checkpoint every N steps
  eval_interval: 500            # Evaluate every N steps

# Inference Configuration
inference:
  max_new_tokens: 256           # Maximum new tokens to generate
  temperature: 1.0              # Sampling temperature
  top_k: 0                      # Top-k sampling (0 to disable)
  top_p: 0.9                    # Top-p (nucleus) sampling
  repetition_penalty: 1.0       # Repetition penalty
  use_recurrent: true           # Use efficient recurrent mode

# Hardware Configuration
hardware:
  dtype: "bfloat16"             # Data type (float32, float16, bfloat16)
  compile: false                # Use torch.compile
  use_cuda_kernel: false        # Use CUDA kernel for WKV

# Model Size Presets
# Uncomment to use a preset configuration

# rwkv_small:
#   hidden_dim: 768
#   num_layers: 12
#   # ~169M parameters

# rwkv_medium:
#   hidden_dim: 1024
#   num_layers: 24
#   # ~430M parameters

# rwkv_large:
#   hidden_dim: 2048
#   num_layers: 24
#   # ~1.5B parameters

# rwkv_xl:
#   hidden_dim: 2560
#   num_layers: 32
#   # ~3B parameters

# rwkv_xxl:
#   hidden_dim: 4096
#   num_layers: 32
#   # ~7B parameters

# rwkv_world:
#   hidden_dim: 5120
#   num_layers: 40
#   # ~14B parameters

# Logging
logging:
  log_interval: 10              # Log every N steps
  wandb_project: null           # Weights & Biases project (null to disable)
  wandb_run_name: null          # W&B run name

# Data
data:
  train_data_path: null         # Path to training data
  val_data_path: null           # Path to validation data
  tokenizer: "rwkv_world"       # Tokenizer to use (rwkv_world, gpt2, etc.)
