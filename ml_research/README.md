# ML Research Module

A comprehensive index and implementation hub for machine learning methods spanning from 1943 to present day.

## Overview

The `ml_research` module serves as both a **research encyclopedia** and an **active development workspace** for machine learning architectures. It provides:

- **Historical Index**: 200+ methods organized by era, category, and lineage
- **Method Registry**: Central database with papers, benchmarks, and relationships
- **Modern Implementations**: Working implementations of cutting-edge 2023-2025 architectures
- **Application Techniques**: Composable patterns for deploying ML models effectively

This module enables the system to understand the evolution of ML methods, select appropriate architectures for tasks, and apply them using proven techniques.

---

## Module Structure

```
ml_research/
â”œâ”€â”€ foundations/          # Era 1: 1943-1980 - Birth of neural networks
â”‚   â”œâ”€â”€ mcculloch_pitts.py      # First computational neuron model (1943)
â”‚   â”œâ”€â”€ perceptron.py           # Rosenblatt's perceptron (1958)
â”‚   â”œâ”€â”€ adaline_madaline.py     # Adaptive linear elements (1960)
â”‚   â”œâ”€â”€ hebbian_learning.py     # Hebb's learning rule
â”‚   â”œâ”€â”€ hopfield_network.py     # Associative memory networks (1982)
â”‚   â””â”€â”€ backpropagation_early.py # Early backprop concepts
â”‚
â”œâ”€â”€ classical/            # Era 2: 1980-2006 - Foundations solidify
â”‚   â”œâ”€â”€ mlp.py                  # Multi-layer perceptrons
â”‚   â”œâ”€â”€ backprop_rumelhart.py   # Modern backpropagation (1986)
â”‚   â”œâ”€â”€ cnn_lecun.py            # LeNet and early CNNs
â”‚   â”œâ”€â”€ svm.py                  # Support Vector Machines
â”‚   â”œâ”€â”€ rbf_networks.py         # Radial Basis Function networks
â”‚   â”œâ”€â”€ autoencoders.py         # Auto-associative networks
â”‚   â”œâ”€â”€ recurrent/              # RNN, LSTM, GRU
â”‚   â”œâ”€â”€ ensemble/               # Random Forest, Boosting, XGBoost
â”‚   â””â”€â”€ probabilistic/          # Boltzmann machines, RBM, DBN
â”‚
â”œâ”€â”€ deep_learning/        # Era 3: 2006-2017 - Deep learning revolution
â”‚   â”œâ”€â”€ alexnet.py              # ImageNet breakthrough (2012)
â”‚   â”œâ”€â”€ vgg.py                  # Very deep networks
â”‚   â”œâ”€â”€ resnet.py               # Residual connections
â”‚   â”œâ”€â”€ inception.py            # Inception modules
â”‚   â”œâ”€â”€ densenet.py             # Dense connections
â”‚   â”œâ”€â”€ batch_norm.py           # Batch normalization
â”‚   â”œâ”€â”€ dropout.py              # Regularization technique
â”‚   â”œâ”€â”€ seq2seq.py              # Sequence-to-sequence models
â”‚   â”œâ”€â”€ unet.py                 # U-Net for segmentation
â”‚   â”œâ”€â”€ generative/             # GAN, VAE, diffusion models
â”‚   â””â”€â”€ object_detection/       # R-CNN, YOLO, SSD
â”‚
â”œâ”€â”€ attention/            # Era 4: 2017-present - Transformer era
â”‚   â”œâ”€â”€ attention_mechanism.py  # Core attention concepts
â”‚   â”œâ”€â”€ self_attention.py       # Self-attention implementation
â”‚   â”œâ”€â”€ transformer.py          # "Attention Is All You Need"
â”‚   â”œâ”€â”€ positional_encoding.py  # Position representations
â”‚   â”œâ”€â”€ language_models/        # GPT, BERT, T5, LLaMA, Mistral, Claude
â”‚   â”œâ”€â”€ vision_transformers/    # ViT, DeiT, Swin, SAM
â”‚   â”œâ”€â”€ multimodal/             # CLIP, Flamingo, LLaVA, GPT-4V
â”‚   â””â”€â”€ efficient_attention/    # Flash, Sparse, Linear, Mamba
â”‚
â”œâ”€â”€ novel/                # Era 5: 2023+ - Emerging methods
â”‚   â”œâ”€â”€ state_space/            # S4, Mamba, SSM variants
â”‚   â”œâ”€â”€ mixture_of_experts/     # MoE architectures
â”‚   â”œâ”€â”€ world_models/           # World modeling approaches
â”‚   â”œâ”€â”€ emerging/               # KAN, Liquid Networks
â”‚   â”œâ”€â”€ neural_architecture_search/  # NAS, ENAS, DARTS
â”‚   â””â”€â”€ neuro_symbolic/         # Neural Turing, differentiable programming
â”‚
â”œâ”€â”€ reinforcement/        # Reinforcement Learning methods
â”‚   â”œâ”€â”€ classical/              # Q-learning, SARSA, TD, Policy Gradient
â”‚   â”œâ”€â”€ deep_rl/                # DQN, A3C, PPO, SAC, DDPG, Rainbow
â”‚   â””â”€â”€ rlhf.py                 # RL from Human Feedback
â”‚
â”œâ”€â”€ optimization/         # Training methods
â”‚   â”œâ”€â”€ gradient_descent.py     # Core optimization
â”‚   â”œâ”€â”€ adaptive/               # Adam, AdamW, Lion, LAMB, etc.
â”‚   â””â”€â”€ learning_rate/          # Schedulers, LR finder
â”‚
â”œâ”€â”€ core/                 # Registry and taxonomy system
â”‚   â”œâ”€â”€ method_registry.py      # Central method index
â”‚   â”œâ”€â”€ taxonomy.py             # Classification system
â”‚   â”œâ”€â”€ timeline.py             # Historical progression
â”‚   â”œâ”€â”€ lineage_tracker.py      # Evolution and influence
â”‚   â”œâ”€â”€ paper_index.py          # Landmark papers database
â”‚   â””â”€â”€ benchmark_tracker.py    # SOTA tracking
â”‚
â”œâ”€â”€ modern_dev/           # ACTIVE DEVELOPMENT - 2023-2025 architectures
â”‚   â”œâ”€â”€ orchestrator/           # Dynamic architecture selection
â”‚   â”œâ”€â”€ ctm/                    # Continuous Thought Machine
â”‚   â”œâ”€â”€ jepa/                   # Joint Embedding Predictive Architecture
â”‚   â”œâ”€â”€ ttt/                    # Test-Time Training
â”‚   â”œâ”€â”€ mamba_impl/             # Mamba SSM implementation
â”‚   â”œâ”€â”€ rwkv/                   # RWKV-7 "Goose"
â”‚   â”œâ”€â”€ xlstm/                  # Extended LSTM
â”‚   â”œâ”€â”€ hyena/                  # Long convolutions
â”‚   â”œâ”€â”€ griffin/                # Gated linear recurrence
â”‚   â”œâ”€â”€ titans/                 # Meta in-context memory
â”‚   â”œâ”€â”€ flow_matching/          # Optimal transport generation
â”‚   â”œâ”€â”€ consistency_models/     # Fast diffusion sampling
â”‚   â””â”€â”€ ring_attention/         # Infinite context attention
â”‚
â”œâ”€â”€ ml_techniques/        # Application patterns and techniques
â”‚   â”œâ”€â”€ decomposition/          # Task breakdown methods
â”‚   â”œâ”€â”€ prompting/              # CoT, ToT, GoT, few-shot
â”‚   â”œâ”€â”€ agentic/                # ReAct, tool calling, multi-agent
â”‚   â”œâ”€â”€ memory/                 # RAG, episodic memory, compression
â”‚   â”œâ”€â”€ code_synthesis/         # Program synthesis, RLM, self-debugging
â”‚   â”œâ”€â”€ orchestration/          # Routing, ensembles, pipelines
â”‚   â”œâ”€â”€ verification/           # Self-eval, CoVe, constitutional
â”‚   â””â”€â”€ optimization/           # DSPy, automatic prompt engineering
â”‚
â””â”€â”€ config/               # Configuration files
    â”œâ”€â”€ methods.yaml            # Method definitions
    â”œâ”€â”€ papers.yaml             # Paper references
    â”œâ”€â”€ benchmarks.yaml         # Benchmark data
    â””â”€â”€ lineages.yaml           # Method relationships
```

---

## Modern Dev Section

The `modern_dev/` directory contains **active development implementations** of cutting-edge ML architectures from 2023-2025. Unlike the research index modules, these are intended to become working implementations with command-line interfaces.

### Purpose

- Provide working implementations of the latest sequence modeling and generative architectures
- Enable the system to dynamically select and employ appropriate architectures for tasks
- Support research and experimentation with emerging methods
- Create modular, swappable implementations for different use cases

### Indexed Architectures

**Tier 1 - Production Ready** (open source, well-documented):

| Architecture | Organization | Key Innovation |
|-------------|--------------|----------------|
| **CTM** | Sakana AI | Neural dynamics with decoupled internal time, emergent adaptive computation |
| **JEPA** | Meta AI | Latent space prediction without pixel reconstruction |
| **xLSTM** | NXAI Lab | Exponential gating + matrix memory for transformer-level performance |
| **RWKV** | RWKV Foundation | RNN efficiency with transformer parallelization, O(n) complexity |
| **Griffin** | Google DeepMind | Gated linear recurrence + local attention hybrid |
| **Mamba** | CMU/Princeton | Selective state spaces with hardware-aware parallel scan |

**Tier 2 - Research Ready** (papers available, some code):

| Architecture | Organization | Key Innovation |
|-------------|--------------|----------------|
| **TTT** | Stanford/NVIDIA | Learnable hidden states that adapt during inference |
| **Hyena** | Hazy Research | Implicit long convolutions for 100x speedup at 64K |
| **Consistency Models** | OpenAI | One-step generation with diffusion quality |
| **Flow Matching** | Meta/DeepMind | Optimal transport paths for fast sampling |
| **Ring Attention** | UC Berkeley | Distributed attention for near-infinite context |
| **Titans** | Google | Meta in-context memory learning at test-time |

### Orchestrator System

The orchestrator provides dynamic architecture selection based on task requirements:

```python
from consciousness.ml_research.modern_dev import Orchestrator, TaskType

# Initialize orchestrator
orch = Orchestrator(device="cuda", max_loaded_models=2)

# Automatic architecture selection
result = orch.run(
    task_type=TaskType.LONG_CONTEXT,
    input_data={"text": "...very long document..."},
    constraints={"context_length": 100000, "max_memory_gb": 16}
)

# Or specify architecture directly
result = orch.run(
    task_type=TaskType.TEXT_GENERATION,
    input_data={"prompt": "Hello"},
    architecture="mamba"
)
```

### Current Status

| Component | Status |
|-----------|--------|
| Architecture index (12) | âœ… Complete |
| Orchestrator with dynamic loading | âœ… Complete |
| Architecture scaffolding (all 12) | âœ… Complete |
| Model presets (tiny/small/medium/large) | âœ… Complete |
| LLM Backend interface | âœ… Complete |
| Integration tests | âœ… Complete |
| Full implementations | ðŸ”„ Stubs ready (requires PyTorch) |

### Architecture Scaffolding

Each architecture now includes:
- `src/model.py` - Core model with config dataclass
- `src/layers.py` - Architecture-specific layers
- `configs/default.yaml` - Default hyperparameters
- `cli/train.py` - Training CLI
- `cli/infer.py` - Inference CLI
- `tests/test_model.py` - Basic tests
- `README.md` - Implementation status and requirements

---

## ML Techniques Section

The `ml_techniques/` module provides composable **application patterns** for using ML models effectively. These are not architectures but ways to structure model usage for complex tasks.

### Categories

| Category | Techniques | Description |
|----------|------------|-------------|
| **Decomposition** | 3 | Breaking complex tasks into manageable parts (recursive, least-to-most, hierarchical) |
| **Prompting** | 10 | Structured input formulation (CoT, ToT, GoT, SoT, Step-Back, Analogical, CoS, self-consistency, few-shot) |
| **Agentic** | 10 | Autonomous execution patterns (ReAct, LATS, ReWOO, tool calling, Toolformer, CRITIC, multi-agent, reflexion, planning, inner monologue) |
| **Memory** | 3 | Context and knowledge management (RAG, episodic memory, compression) |
| **Code Synthesis** | 7 | Code generation patterns (RLM, PoT, PAL, Scratchpad, program synthesis, self-debugging, code-as-policy) |
| **Orchestration** | 3 | Multi-component coordination (routing, ensembles, hooks) |
| **Verification** | 6 | Output validation (self-eval, Self-Refine, RCI, CoVe, constitutional, debate) |
| **Optimization** | 7 | Technique improvement (DSPy, OPRO, Meta-Prompting, Active Prompting, PromptBreeder, EvoPrompt, APE) |

**Total: 49 indexed techniques**

### Composable Pipeline System

Techniques can be composed using intuitive operators:

```python
from consciousness.ml_research.ml_techniques import Pipeline, compose
from consciousness.ml_research.ml_techniques import TechniqueBase

# Sequential composition with >>
pipeline = technique_a >> technique_b >> technique_c

# Parallel composition with |
parallel = technique_a | technique_b | technique_c

# Or explicit composition
pipeline = compose([
    RecursiveDecomposition(max_depth=3),
    ChainOfThought(cot_trigger="Let's think step by step."),
    ToolCalling(tools=[calculator, search]),
    SelfConsistency(samples=5),
])

result = pipeline.run(complex_task)
```

### Configuration-Driven Usage

```python
from consciousness.ml_research.ml_techniques import load_config

# Load from YAML configuration
config = load_config("configs/research_pipeline.yaml")
```

---

## Goals

### Research Index Goals
- Document the complete evolution of ML methods from 1943 to present
- Track method lineages and influence relationships
- Maintain paper references and benchmark results
- Enable historical understanding and method comparison

### Modern Dev Goals
- **Dynamic Selection**: Enable the system to automatically select appropriate architectures based on task requirements, constraints, and available resources
- **Modular Implementations**: Provide swappable implementations that follow a consistent interface
- **Research Support**: Support experimentation with architecture modifications and hybrid approaches
- **Production Path**: Create a clear path from research prototype to production deployment

### ML Techniques Goals
- **Composability**: Allow techniques to be combined freely for complex pipelines
- **Configurability**: Support YAML/JSON configuration for reproducibility
- **Pluggability**: Work with any compatible model backend
- **Traceability**: Provide execution traces for debugging and analysis

---

## Getting Started

### Basic Usage - Research Index

```python
from consciousness.ml_research import (
    MethodRegistry,
    Timeline,
    LineageTracker,
    MethodEra,
)

# Get all indexed methods
all_methods = MethodRegistry.get_all()
print(f"Total methods indexed: {len(all_methods)}")

# Get methods by era
attention_methods = MethodRegistry.get_by_era(MethodEra.ATTENTION)

# Show method lineage
LineageTracker.show_lineage('transformer')

# View timeline for an era
Timeline.show_era('deep_learning')

# Search for methods
results = MethodRegistry.search('attention')
```

### Using Modern Architectures

```python
from consciousness.ml_research.modern_dev import (
    Orchestrator,
    TaskType,
    get_architecture_info,
    list_architectures,
    ImplementationTier,
)

# Get information about an architecture
ctm_info = get_architecture_info('ctm')
print(f"CTM: {ctm_info.key_innovation}")

# List production-ready architectures
tier1 = list_architectures(tier=ImplementationTier.TIER_1)

# Use orchestrator for task execution
orch = Orchestrator()

# Run with automatic architecture selection
result = orch.run(
    task_type=TaskType.REASONING,
    input_data={"problem": "Solve this puzzle..."},
)

# Check what architectures are available for a task
available = orch.list_available(task_type=TaskType.LONG_CONTEXT)
```

### Applying ML Techniques

```python
from consciousness.ml_research.ml_techniques import (
    get_technique_info,
    list_techniques,
    TechniqueCategory,
    compose,
    Pipeline,
)

# Get technique information
cot_info = get_technique_info('chain_of_thought')
print(f"CoT composable with: {cot_info.composable_with}")

# List techniques by category
prompting_techniques = list_techniques(category=TechniqueCategory.PROMPTING)

# Find composable techniques
from consciousness.ml_research.ml_techniques import get_composable_with
compatible = get_composable_with('react')

# Build a custom pipeline
from consciousness.ml_research.ml_techniques.prompting import ChainOfThought
from consciousness.ml_research.ml_techniques.verification import SelfRefine
from consciousness.ml_research.backends import MockBackend

backend = MockBackend()
cot = ChainOfThought(backend=backend)
result = cot.run("Solve this step by step: What is 15% of 80?")
print(result.output)
```

### Using the LLM Backend Interface

```python
from consciousness.ml_research.backends import (
    MockBackend,
    LocalModelBackend,
    get_backend,
    register_backend,
    list_backends,
)

# Use mock backend for testing (default)
mock = MockBackend()
response = mock.generate("Hello world", temperature=0.7)
embedding = mock.embed("test text")  # Returns 384-dim vector

# Register a custom backend
register_backend("my_mock", mock, set_as_default=True)

# All techniques use the backend interface
from consciousness.ml_research.ml_techniques.prompting import ChainOfThought
from consciousness.ml_research.ml_techniques.memory import RAG
from consciousness.ml_research.ml_techniques.agentic import ReAct

cot = ChainOfThought(backend=mock)
rag = RAG(backend=mock, embedding_backend=mock)
react = ReAct(backend=mock, tools=[...])
```

---

## Roadmap

### Complete
- [x] Research index structure (foundations through novel)
- [x] Core registry system (method_registry, taxonomy, lineage, timeline)
- [x] Modern dev architecture index (12 architectures catalogued)
- [x] Orchestrator with dynamic architecture loading
- [x] ML techniques index (49 techniques across 8 categories)
- [x] Technique base classes and composition system
- [x] Architecture scaffolding for all modern_dev modules
- [x] LLM Backend interface (Mock, Local, Anthropic, OpenAI)
- [x] Integration tests for end-to-end validation
- [x] Model presets (tiny/small/medium/large)
- [x] README documentation for all 12 architectures

### In Progress
- [ ] Connect real LLM APIs (Anthropic/OpenAI) for technique backends
- [ ] Parallel scan implementation for Mamba performance
- [ ] CUDA/Triton kernels for production speed

### Planned
- [ ] Pretrained model weight loading
- [ ] Benchmark integration for architecture comparison
- [ ] Configuration-driven pipeline loading from YAML
- [ ] Web UI for technique composition

---

## Architecture

### Design Principles

1. **Separation of Concerns**: Index/registry is separate from implementations
2. **Consistent Interfaces**: All architectures inherit from `ArchitectureBase`
3. **Lazy Loading**: Models load only when needed
4. **Resource Management**: Orchestrator manages memory with LRU eviction
5. **Composability**: Techniques combine freely via operators

### Key Classes

```python
# Core taxonomy
from consciousness.ml_research.core import (
    MethodEra,      # FOUNDATIONS, CLASSICAL, DEEP_LEARNING, ATTENTION, NOVEL
    MethodCategory, # SUPERVISED, UNSUPERVISED, GENERATIVE, etc.
    MLMethod,       # Method metadata dataclass
)

# Modern dev
from consciousness.ml_research.modern_dev import (
    ArchitectureBase,    # Base class for implementations
    ArchitectureIndex,   # Architecture metadata
    Orchestrator,        # Task routing and execution
    TaskType,            # Task categorization
    TaskSpec,            # Task specification
    TaskResult,          # Execution result
)

# ML techniques
from consciousness.ml_research.ml_techniques import (
    TechniqueBase,       # Base class for techniques
    TechniqueCategory,   # PROMPTING, AGENTIC, MEMORY, etc.
    TechniqueResult,     # Execution result
    Pipeline,            # Sequential composition
    ParallelComposition, # Parallel execution
)

# LLM backends
from consciousness.ml_research.backends import (
    LLMBackend,          # Abstract base class
    MockBackend,         # Testing backend
    LocalModelBackend,   # PyTorch model wrapper
    BackendRegistry,     # Backend management
    get_backend,         # Get backend by name
)
```

---

## Contributing

When adding new methods or implementations:

1. **Research Index**: Add to appropriate era module, include paper reference and lineage
2. **Modern Dev**:
   - Create subdirectory with `__init__.py`, `src/`, `configs/`, `tests/`
   - Implement `ArchitectureBase` subclass
   - Register in `ARCHITECTURE_CAPABILITIES`
3. **ML Techniques**:
   - Add `TechniqueIndex` entry to registry
   - Implement `TechniqueBase` subclass
   - Document composability with other techniques

---

## Version

- **Module Version**: 1.0.0
- **Modern Dev Version**: 0.1.0 (development)
- **ML Techniques Version**: 0.1.0 (indexed)
